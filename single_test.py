import os
import torch
import torch.nn as nn
from torchvision import models, transforms
from PIL import Image
import numpy as np
from tqdm import tqdm
import warnings
from transformers import CLIPModel, CLIPProcessor
from peft import LoraConfig, get_peft_model, TaskType
import glob
import time

warnings.filterwarnings('ignore')

# --- é…ç½®å‚æ•° ---
# æ¨¡å‹è·¯å¾„é…ç½®
MODEL_PATHS = {
    'inception': "./lung_cancer_data/829_best_model.pth",
    'resnet': "./lung_cancer_data/resnet50_best_model.pth", 
    'clip_lora': "./lung_cancer_data/clip_lora_best_model.pth"
}

# CLIPæ¨¡å‹é…ç½®
LOCAL_CLIP_MODEL_PATH = "/Users/huangxh/Documents/DMECL/clip-vit-base-patch32-local"

# ç±»åˆ«å®šä¹‰ï¼ˆä¸è®­ç»ƒæ—¶ä¿
CLASSES = ['lung_aca', 'lung_n', 'lung_scc']  # è‚ºè…ºç™Œã€æ­£å¸¸ã€è‚ºé³ç™Œ
NUM_CLASSES = len(CLASSES)

# ç±»åˆ«ä¸­æ–‡åç§°æ˜ å°„
CLASS_NAMES_CN = {
    'lung_aca': 'è‚ºè…ºç™Œ',
    'lung_n': 'æ­£å¸¸',
    'lung_scc': 'è‚ºé³ç™Œ'
}

# LoRAé…ç½®ï¼ˆä¸è®­ç»ƒæ—¶ä¿æŒä¸€è‡´ï¼‰
LORA_CONFIG = {
    "r": 32,
    "lora_alpha": 64,
    "lora_dropout": 0.05,
    "target_modules": [
        "q_proj", "k_proj", "v_proj", "out_proj",
        "fc1", "fc2"
    ]
}

# è®¾å¤‡æ£€æµ‹
def get_device():
    """è·å–æœ€ä½³å¯ç”¨è®¾å¤‡"""
    if torch.backends.mps.is_available():
        return torch.device("mps")
    elif torch.cuda.is_available():
        return torch.device("cuda")
    else:
        return torch.device("cpu")

DEVICE = get_device()
print(f"Using device: {DEVICE}")

# --- æ¨¡å‹å®šä¹‰ï¼ˆä¸vote_fix.pyä¿æŒä¸€è‡´ï¼‰ ---
def load_inception_model():
    """åŠ è½½Inception v3æ¨¡å‹ï¼Œå…¼å®¹ä¸åŒç‰ˆæœ¬çš„torchvision"""
    try:
        model = models.inception_v3(weights=models.Inception_V3_Weights.DEFAULT)
    except AttributeError:
        model = models.inception_v3(pretrained=True)
    return model

def create_inception_model(num_classes=NUM_CLASSES):
    """åˆ›å»ºä¸è®­ç»ƒæ—¶å®Œå…¨ä¸€è‡´çš„Inceptionæ¨¡å‹"""
    model = load_inception_model()
    
    # æ›¿æ¢åˆ†ç±»å™¨ï¼ˆä¸è®­ç»ƒæ—¶ä¿æŒä¸€è‡´ï¼‰
    num_ftrs = model.fc.in_features
    model.fc = nn.Linear(num_ftrs, num_classes)
    
    # æ›¿æ¢è¾…åŠ©åˆ†ç±»å™¨ï¼ˆä¸è®­ç»ƒæ—¶ä¿æŒä¸€è‡´ï¼‰
    if hasattr(model, 'AuxLogits'):
        num_ftrs_aux = model.AuxLogits.fc.in_features
        model.AuxLogits.fc = nn.Linear(num_ftrs_aux, num_classes)
    
    return model

class ResNet50Classifier(nn.Module):
    """ResNet50 + è‡ªå®šä¹‰åˆ†ç±»å¤´æ¨¡å‹"""
    def __init__(self, num_classes=NUM_CLASSES, pretrained=False):
        super(ResNet50Classifier, self).__init__()
        
        self.backbone = models.resnet50(pretrained=pretrained)
        self.feature_dim = self.backbone.fc.in_features
        self.backbone = nn.Sequential(*list(self.backbone.children())[:-1])
        
        self.classifier = nn.Sequential(
            nn.AdaptiveAvgPool2d((1, 1)),
            nn.Flatten(),
            nn.Dropout(0.5),
            nn.Linear(self.feature_dim, 512),
            nn.ReLU(inplace=True),
            nn.Dropout(0.3),
            nn.Linear(512, 256),
            nn.ReLU(inplace=True),
            nn.Dropout(0.2),
            nn.Linear(256, num_classes)
        )
    
    def forward(self, x):
        features = self.backbone(x)
        logits = self.classifier(features)
        return logits

class CLIPLoRAClassifier(nn.Module):
    """CLIP Vision Encoder + LoRA + å¢å¼ºåˆ†ç±»å¤´æ¨¡å‹"""
    def __init__(self, clip_model_name=LOCAL_CLIP_MODEL_PATH, num_classes=NUM_CLASSES, 
                 lora_config=LORA_CONFIG):
        super(CLIPLoRAClassifier, self).__init__()
        
        self.base_clip_model = CLIPModel.from_pretrained(clip_model_name)
        self.processor = CLIPProcessor.from_pretrained(clip_model_name)
        
        # å†»ç»“æ‰€æœ‰CLIPå‚æ•°
        for param in self.base_clip_model.parameters():
            param.requires_grad = False
        
        # é…ç½®LoRA
        self.lora_config = LoraConfig(
            r=lora_config["r"],
            lora_alpha=lora_config["lora_alpha"],
            target_modules=lora_config["target_modules"],
            lora_dropout=lora_config["lora_dropout"],
            bias="none",
            task_type=TaskType.FEATURE_EXTRACTION,
        )
        
        self.clip_model = get_peft_model(self.base_clip_model, self.lora_config)
        self.clip_hidden_size = self.base_clip_model.config.vision_config.hidden_size
        
        self.classifier = nn.Sequential(
            nn.Dropout(0.1),
            nn.Linear(self.clip_hidden_size, 1024),
            nn.ReLU(),
            nn.BatchNorm1d(1024),
            nn.Dropout(0.2),
            nn.Linear(1024, 512),
            nn.ReLU(),
            nn.BatchNorm1d(512),
            nn.Dropout(0.1),
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(256, num_classes)
        )
    
    def forward(self, pixel_values):
        vision_outputs = self.clip_model.vision_model(pixel_values=pixel_values)
        pooled_output = vision_outputs.pooler_output
        logits = self.classifier(pooled_output)
        return logits

# --- æ™ºèƒ½æ¨¡å‹åŠ è½½å‡½æ•° ---
def smart_load_checkpoint(model, checkpoint_path, model_name):
    """æ™ºèƒ½åŠ è½½checkpointï¼Œè‡ªåŠ¨å¤„ç†ä¸åŒçš„ä¿å­˜æ ¼å¼"""
    try:
        checkpoint = torch.load(checkpoint_path, map_location=DEVICE)
        
        # æ£€æŸ¥checkpointçš„æ ¼å¼
        if isinstance(checkpoint, dict):
            if 'state_dict' in checkpoint:
                print(f"Loading {model_name} with 'state_dict' format...")
                state_dict = checkpoint['state_dict']
            elif 'model_state_dict' in checkpoint:
                print(f"Loading {model_name} with 'model_state_dict' format...")
                state_dict = checkpoint['model_state_dict']
            else:
                print(f"Loading {model_name} with direct state_dict format...")
                state_dict = checkpoint
        else:
            print(f"Loading {model_name} with direct model format...")
            state_dict = checkpoint
        
        # å°è¯•åŠ è½½state_dict
        missing_keys, unexpected_keys = model.load_state_dict(state_dict, strict=False)
        
        if len(missing_keys) > 0:
            print(f"âš ï¸  Missing keys in {model_name}: {len(missing_keys)} keys")
        if len(unexpected_keys) > 0:
            print(f"âš ï¸  Unexpected keys in {model_name}: {len(unexpected_keys)} keys")
        
        print(f"âœ… {model_name} model loaded successfully!")
        return True
        
    except Exception as e:
        print(f"âŒ Error loading {model_name} model: {e}")
        return False

# --- å›¾åƒé¢„å¤„ç†å‡½æ•° ---
def get_transforms():
    """è·å–å„æ¨¡å‹çš„é¢„å¤„ç†å˜æ¢"""
    transforms_dict = {
        'inception': transforms.Compose([
            transforms.Resize(320),
            transforms.CenterCrop(299),
            transforms.ToTensor(),
            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
        ]),
        'resnet': transforms.Compose([
            transforms.Resize((256, 256)),
            transforms.CenterCrop(224),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ]),
        'clip_lora': transforms.Compose([
            transforms.Resize(256),
            transforms.CenterCrop(224)
        ])
    }
    return transforms_dict

# --- å•æ¨¡å‹æµ‹è¯•å™¨ç±» ---
class SingleModelTester:
    """å•æ¨¡å‹æµ‹è¯•å™¨"""
    
    def __init__(self, model_name, model_path):
        self.model_name = model_name
        self.model_path = model_path
        self.device = DEVICE
        self.transforms = get_transforms()
        self.model = None
        self.clip_processor = None
        
        # åŠ è½½æ¨¡å‹
        self._load_model()
    
    def _load_model(self):
        """åŠ è½½æŒ‡å®šçš„æ¨¡å‹"""
        if not os.path.exists(self.model_path):
            raise FileNotFoundError(f"Model file not found: {self.model_path}")
        
        print(f"\nğŸ”„ Loading {self.model_name} model...")
        
        try:
            if self.model_name == 'inception':
                self.model = create_inception_model()
            elif self.model_name == 'resnet':
                self.model = ResNet50Classifier()
            elif self.model_name == 'clip_lora':
                self.model = CLIPLoRAClassifier()
                self.clip_processor = CLIPProcessor.from_pretrained(LOCAL_CLIP_MODEL_PATH)
            else:
                raise ValueError(f"Unknown model name: {self.model_name}")
            
            # åŠ è½½æƒé‡
            success = smart_load_checkpoint(self.model, self.model_path, self.model_name)
            if not success:
                raise RuntimeError(f"Failed to load {self.model_name} model weights")
            
            # ç§»åŠ¨åˆ°è®¾å¤‡å¹¶è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
            self.model = self.model.to(self.device)
            self.model.eval()
            
            print(f"âœ… {self.model_name} model loaded and ready for testing")
            
        except Exception as e:
            print(f"âŒ Error loading {self.model_name} model: {e}")
            raise
    
    def _preprocess_image(self, image_path):
        """é¢„å¤„ç†å›¾åƒ"""
        try:
            image = Image.open(image_path).convert('RGB')
        except Exception as e:
            print(f"Error loading image {image_path}: {e}")
            return None
        
        if self.model_name == 'clip_lora' and self.clip_processor is not None:
            # CLIPæ¨¡å‹ä½¿ç”¨processor
            processed = self.clip_processor(images=image, return_tensors="pt")
            return processed['pixel_values'].to(self.device)
        else:
            # Inceptionå’ŒResNetä½¿ç”¨transforms
            transform = self.transforms[self.model_name]
            image_tensor = transform(image).unsqueeze(0)
            return image_tensor.to(self.device)
    
    def _predict_single_image(self, image_tensor):
        """å¯¹å•å¼ å›¾åƒè¿›è¡Œé¢„æµ‹"""
        with torch.no_grad():
            # ç‰¹æ®Šå¤„ç†Inceptionæ¨¡å‹çš„è®­ç»ƒæ¨¡å¼è¾“å‡º
            if self.model_name == 'inception' and hasattr(self.model, 'AuxLogits'):
                self.model.eval()  # ç¡®ä¿åœ¨è¯„ä¼°æ¨¡å¼
                outputs = self.model(image_tensor)
                # åœ¨è¯„ä¼°æ¨¡å¼ä¸‹ï¼ŒInceptionåªè¿”å›ä¸»è¾“å‡º
                if isinstance(outputs, tuple):
                    outputs = outputs[0]  # å–ä¸»è¾“å‡º
            else:
                outputs = self.model(image_tensor)
            
            probabilities = torch.nn.functional.softmax(outputs, dim=1)
            _, predicted = torch.max(outputs, 1)
            
            predicted_class_idx = predicted.item()
            predicted_class = CLASSES[predicted_class_idx]
            confidence = probabilities[0][predicted_class_idx].item()
            
            return predicted_class, confidence, probabilities[0].cpu().numpy()
    
    def test_on_dataset(self, image_paths):
        """åœ¨æ•°æ®é›†ä¸Šæµ‹è¯•æ¨¡å‹"""
        print(f"\n{'='*70}")
        print(f"ğŸ§ª Testing {self.model_name.upper()} Model on {len(image_paths)} images")
        print(f"{'='*70}")
        
        correct_predictions = 0
        total_predictions = 0
        
        # æŒ‰ç±»åˆ«ç»Ÿè®¡
        class_stats = {class_name: {'correct': 0, 'total': 0} for class_name in CLASSES}
        
        # è®°å½•é¢„æµ‹æ—¶é—´
        start_time = time.time()
        
        for image_path in tqdm(image_paths, desc=f"Testing {self.model_name}"):
            try:
                # é¢„å¤„ç†å›¾åƒ
                image_tensor = self._preprocess_image(image_path)
                if image_tensor is None:
                    continue
                
                # é¢„æµ‹
                pred_class, confidence, probs = self._predict_single_image(image_tensor)
                if pred_class is not None:
                    # ä»æ–‡ä»¶è·¯å¾„æ¨æ–­çœŸå®æ ‡ç­¾
                    true_label = self._extract_true_label(image_path)
                    
                    if true_label:
                        class_stats[true_label]['total'] += 1
                        total_predictions += 1
                        
                        if pred_class == true_label:
                            class_stats[true_label]['correct'] += 1
                            correct_predictions += 1
                
            except Exception as e:
                print(f"Error processing {image_path}: {e}")
                continue
        
        end_time = time.time()
        test_time = end_time - start_time
        
        # æ‰“å°è¯¦ç»†çš„å‡†ç¡®ç‡ç»Ÿè®¡
        self._print_test_results(class_stats, total_predictions, correct_predictions, test_time)
        
        return class_stats, total_predictions, correct_predictions
    
    def _extract_true_label(self, image_path):
        """ä»æ–‡ä»¶è·¯å¾„æå–çœŸå®æ ‡ç­¾"""
        for class_name in CLASSES:
            if class_name in image_path:
                return class_name
        return None
    
    def _print_test_results(self, class_stats, total_predictions, correct_predictions, test_time):
        """æ‰“å°æµ‹è¯•ç»“æœ"""
        print(f"\nğŸ“Š {self.model_name.upper()} MODEL - TEST RESULTS")
        print(f"="*60)
        
        # æ€»ä½“å‡†ç¡®ç‡
        if total_predictions > 0:
            overall_accuracy = correct_predictions / total_predictions
            print(f"ğŸ¯ Overall Accuracy: {overall_accuracy:.4f} ({correct_predictions}/{total_predictions})")
        else:
            print("ğŸ¯ Overall Accuracy: No valid predictions")
        
        print(f"â±ï¸  Test Time: {test_time:.2f} seconds")
        print(f"ğŸš€ Speed: {total_predictions/test_time:.2f} images/second")
        
        print(f"\nğŸ“ˆ Accuracy by Class:")
        print("-" * 50)
        
        for class_name in CLASSES:
            stats = class_stats[class_name]
            if stats['total'] > 0:
                accuracy = stats['correct'] / stats['total']
                class_name_cn = CLASS_NAMES_CN.get(class_name, class_name)
                print(f"  {class_name_cn:8s} ({class_name:8s}): {accuracy:.4f} ({stats['correct']:3d}/{stats['total']:3d})")
            else:
                class_name_cn = CLASS_NAMES_CN.get(class_name, class_name)
                print(f"  {class_name_cn:8s} ({class_name:8s}): No samples")
        
        print("="*60)

# --- æ•°æ®é›†åŠ è½½å‡½æ•° ---
def load_test_dataset():
    """åŠ è½½æµ‹è¯•æ•°æ®é›†"""
    dataset_path = "/Users/huangxh/Documents/DMECL/LC25000"
    
    if not os.path.exists(dataset_path):
        raise FileNotFoundError(f"Dataset directory not found: {dataset_path}")
    
    test_images = []
    
    print(f"ğŸ“ Loading test images from {dataset_path}")
    
    for class_name in CLASSES:
        class_dir = os.path.join(dataset_path, class_name)
        if os.path.exists(class_dir):
            # è·å–è¯¥ç±»åˆ«çš„æ‰€æœ‰å›¾ç‰‡
            class_images = []
            for ext in ['*.jpeg', '*.jpg', '*.png']:
                class_images.extend(glob.glob(os.path.join(class_dir, ext)))
            
            test_images.extend(class_images)
            class_name_cn = CLASS_NAMES_CN.get(class_name, class_name)
            print(f"  {class_name_cn} ({class_name}): {len(class_images)} images")
    
    print(f"\nğŸ” Total images to test: {len(test_images)}")
    return test_images

# --- æ¯”è¾ƒç»“æœå‡½æ•° ---
def compare_model_results(results):
    """æ¯”è¾ƒæ‰€æœ‰æ¨¡å‹çš„æµ‹è¯•ç»“æœ"""
    print(f"\n{'='*80}")
    print(f"ğŸ“Š MODEL COMPARISON - SUMMARY RESULTS")
    print(f"{'='*80}")
    
    # è¡¨å¤´
    print(f"{'Model':<15} {'Overall Acc':<12} {'è‚ºè…ºç™Œ Acc':<12} {'æ­£å¸¸ Acc':<10} {'è‚ºé³ç™Œ Acc':<12}")
    print("-" * 80)
    
    # æ¯ä¸ªæ¨¡å‹çš„ç»“æœ
    for model_name, (class_stats, total_preds, correct_preds) in results.items():
        overall_acc = correct_preds / total_preds if total_preds > 0 else 0
        
        # è®¡ç®—å„ç±»åˆ«å‡†ç¡®ç‡
        class_accs = {}
        for class_name in CLASSES:
            stats = class_stats[class_name]
            if stats['total'] > 0:
                class_accs[class_name] = stats['correct'] / stats['total']
            else:
                class_accs[class_name] = 0
        
        print(f"{model_name:<15} {overall_acc:<12.4f} {class_accs['lung_aca']:<12.4f} "
              f"{class_accs['lung_n']:<10.4f} {class_accs['lung_scc']:<12.4f}")
    
    print("="*80)
    
    # æ‰¾å‡ºæœ€ä½³æ¨¡å‹
    best_model = max(results.items(), key=lambda x: x[1][1]/x[1][2] if x[1][2] > 0 else 0)
    best_acc = best_model[1][1] / best_model[1][2] if best_model[1][2] > 0 else 0
    
    print(f"\nğŸ† Best Overall Performance: {best_model[0].upper()} ({best_acc:.4f})")
    
    # å„ç±»åˆ«æœ€ä½³æ€§èƒ½
    print(f"\nğŸ¯ Best Performance by Class:")
    for class_name in CLASSES:
        class_name_cn = CLASS_NAMES_CN.get(class_name, class_name)
        best_class_model = None
        best_class_acc = 0
        
        for model_name, (class_stats, _, _) in results.items():
            stats = class_stats[class_name]
            if stats['total'] > 0:
                acc = stats['correct'] / stats['total']
                if acc > best_class_acc:
                    best_class_acc = acc
                    best_class_model = model_name
        
        if best_class_model:
            print(f"  {class_name_cn}: {best_class_model.upper()} ({best_class_acc:.4f})")

# --- ä¸»å‡½æ•° ---
def main():
    """ä¸»å‡½æ•°"""
    try:
        print("="*80)
        print("ğŸ§ª Individual Model Testing on LC25000 Dataset")
        print("="*80)
        print(f"ğŸ“± Device: {DEVICE}")
        print(f"ğŸ¯ Models to test: {list(MODEL_PATHS.keys())}")
        
        # åŠ è½½æµ‹è¯•æ•°æ®é›†
        test_images = load_test_dataset()
        
        if len(test_images) == 0:
            print("âŒ No test images found!")
            return
        
        # å­˜å‚¨æ‰€æœ‰æ¨¡å‹çš„æµ‹è¯•ç»“æœ
        all_results = {}
        
        # é€ä¸ªæµ‹è¯•æ¯ä¸ªæ¨¡å‹
        for model_name, model_path in MODEL_PATHS.items():
            if os.path.exists(model_path):
                try:
                    # åˆ›å»ºå•æ¨¡å‹æµ‹è¯•å™¨
                    tester = SingleModelTester(model_name, model_path)
                    
                    # åœ¨æ•°æ®é›†ä¸Šæµ‹è¯•
                    class_stats, total_preds, correct_preds = tester.test_on_dataset(test_images)
                    
                    # ä¿å­˜ç»“æœ
                    all_results[model_name] = (class_stats, total_preds, correct_preds)
                    
                except Exception as e:
                    print(f"âŒ Error testing {model_name}: {e}")
                    continue
            else:
                print(f"âš ï¸  Model file not found: {model_path}")
        
        # æ¯”è¾ƒæ‰€æœ‰æ¨¡å‹çš„ç»“æœ
        if all_results:
            compare_model_results(all_results)
        else:
            print("âŒ No models were successfully tested!")
        
        print(f"\nâœ… Individual model testing completed!")
        
    except KeyboardInterrupt:
        print("\nâš ï¸  Testing interrupted by user")
    except Exception as e:
        print(f"âŒ Error in main function: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()