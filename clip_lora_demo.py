import os
import torch
import torch.nn as nn
from torchvision import transforms
from PIL import Image
import numpy as np
from transformers import CLIPModel, CLIPProcessor
from peft import LoraConfig, get_peft_model, PeftModel, TaskType

# --- é…ç½®å‚æ•° ---
# æ¨¡åž‹è·¯å¾„
MODEL_PATH = "./lung_cancer_data/clip_lora_best_model.pth"
LORA_WEIGHTS_PATH = "./lung_cancer_data/lora_weights"

# CLIPæ¨¡åž‹é…ç½®ï¼ˆä¸Žè®­ç»ƒæ—¶ä¿æŒä¸€è‡´ï¼‰
# CLIP_MODEL_NAME = "openai/clip-vit-base-patch32"  # å»ºè®®ä½¿ç”¨åœ¨çº¿æ¨¡åž‹é¿å…è¿žæŽ¥é—®é¢˜
# å¦‚æžœä½ æœ‰å®Œæ•´çš„æœ¬åœ°æ¨¡åž‹ï¼Œå¯ä»¥ä½¿ç”¨ï¼š
CLIP_MODEL_NAME = "/Users/huangxh/Documents/DMECL/clip-vit-base-patch32-local"

# ä¼˜åŒ–åŽçš„LoRAé…ç½®å‚æ•°ï¼ˆä¸Žè®­ç»ƒæ—¶ä¿æŒä¸€è‡´ï¼‰
LORA_CONFIG = {
    "r": 32,  # å¢žåŠ LoRA rank
    "lora_alpha": 64,  # å¢žåŠ alphaå€¼
    "lora_dropout": 0.05,  # é™ä½Ždropout
    "target_modules": [
        "q_proj", "k_proj", "v_proj", "out_proj",  # attention layers
        "fc1", "fc2"  # feed forward layers
    ]
}

# ç±»åˆ«å®šä¹‰ï¼ˆä¸Žè®­ç»ƒæ—¶ä¿æŒä¸€è‡´ï¼‰
CLASSES = ['lung_aca', 'lung_n', 'lung_scc']  # è‚ºè…ºç™Œã€æ­£å¸¸ã€è‚ºé³žç™Œ
NUM_CLASSES = len(CLASSES)

# ç±»åˆ«ä¸­æ–‡åç§°æ˜ å°„
CLASS_NAMES_CN = {
    'lung_aca': 'è‚ºè…ºç™Œ',
    'lung_n': 'æ­£å¸¸',
    'lung_scc': 'è‚ºé³žç™Œ'
}

# è®¾å¤‡æ£€æµ‹å‡½æ•°ï¼ˆä¸Žè®­ç»ƒè„šæœ¬ä¿æŒä¸€è‡´ï¼‰
def get_device():
    """
    èŽ·å–æœ€ä½³å¯ç”¨è®¾å¤‡ï¼Œä¼˜å…ˆçº§ï¼šMPS > CUDA > CPU
    """
    if torch.backends.mps.is_available():
        return torch.device("mps")
    elif torch.cuda.is_available():
        return torch.device("cuda")
    else:
        return torch.device("cpu")

DEVICE = get_device()
print(f"Using device: {DEVICE}")

# --- å¢žå¼ºçš„CLIP + LoRA æ¨¡åž‹å®šä¹‰ï¼ˆä¸Žè®­ç»ƒè„šæœ¬ä¿æŒä¸€è‡´ï¼‰---
class CLIPLoRAClassifier(nn.Module):
    """
    CLIP Vision Encoder + LoRA + å¢žå¼ºåˆ†ç±»å¤´æ¨¡åž‹
    """
    def __init__(self, clip_model_name=CLIP_MODEL_NAME, num_classes=NUM_CLASSES, 
                 lora_config=LORA_CONFIG, load_lora_from_path=None):
        super(CLIPLoRAClassifier, self).__init__()
        
        # åŠ è½½åŸºç¡€CLIPæ¨¡åž‹
        print("Loading base CLIP model...")
        self.base_clip_model = CLIPModel.from_pretrained(clip_model_name)
        self.processor = CLIPProcessor.from_pretrained(clip_model_name)
        
        # å†»ç»“æ‰€æœ‰CLIPå‚æ•°
        for param in self.base_clip_model.parameters():
            param.requires_grad = False
        
        # é…ç½®LoRA
        self.lora_config = LoraConfig(
            r=lora_config["r"],
            lora_alpha=lora_config["lora_alpha"],
            target_modules=lora_config["target_modules"],
            lora_dropout=lora_config["lora_dropout"],
            bias="none",
            task_type=TaskType.FEATURE_EXTRACTION,
        )
        
        # åº”ç”¨LoRAåˆ°CLIP vision model
        if load_lora_from_path and os.path.exists(load_lora_from_path):
            print(f"Loading LoRA weights from: {load_lora_from_path}")
            self.clip_model = PeftModel.from_pretrained(self.base_clip_model, load_lora_from_path)
        else:
            print("Applying LoRA to CLIP vision model...")
            self.clip_model = get_peft_model(self.base_clip_model, self.lora_config)
        
        # èŽ·å–CLIP vision encoderçš„è¾“å‡ºç»´åº¦
        self.clip_hidden_size = self.base_clip_model.config.vision_config.hidden_size
        
        # å¢žå¼ºçš„åˆ†ç±»å¤´ - ä¸Žè®­ç»ƒæ—¶ä¿æŒä¸€è‡´
        self.classifier = nn.Sequential(
            nn.Dropout(0.1),
            nn.Linear(self.clip_hidden_size, 1024),
            nn.ReLU(),
            nn.BatchNorm1d(1024),
            nn.Dropout(0.2),
            nn.Linear(1024, 512),
            nn.ReLU(),
            nn.BatchNorm1d(512),
            nn.Dropout(0.1),
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(256, num_classes)
        )
        
        print(f"Enhanced CLIP + LoRA hidden size: {self.clip_hidden_size}")
        print(f"Enhanced Classification head: {self.clip_hidden_size} -> 1024 -> 512 -> 256 -> {num_classes}")
    
    def forward(self, pixel_values):
        # é€šè¿‡CLIP vision encoder (with LoRA) æå–ç‰¹å¾
        vision_outputs = self.clip_model.vision_model(pixel_values=pixel_values)
        
        # èŽ·å–pooledè¾“å‡º (CLS tokençš„è¡¨ç¤º)
        pooled_output = vision_outputs.pooler_output  # [batch_size, hidden_size]
        
        # é€šè¿‡å¢žå¼ºçš„åˆ†ç±»å¤´
        logits = self.classifier(pooled_output)
        
        return logits

# --- æ™ºèƒ½è·¯å¾„æ£€æµ‹å‡½æ•° ---
def get_clip_model_path():
    """
    èŽ·å–CLIPæ¨¡åž‹è·¯å¾„ï¼Œä¼˜å…ˆä½¿ç”¨æœ¬åœ°ï¼Œå›žé€€åˆ°åœ¨çº¿
    """
    local_path = "/Users/huangxh/Documents/DMECL/clip-vit-base-patch32-local"
    online_path = "openai/clip-vit-base-patch32"
    
    # æ£€æŸ¥æœ¬åœ°è·¯å¾„æ˜¯å¦å­˜åœ¨ä¸”åŒ…å«å¿…è¦æ–‡ä»¶
    if os.path.exists(local_path):
        config_file = os.path.join(local_path, "config.json")
        preprocessor_file = os.path.join(local_path, "preprocessor_config.json")
        
        if os.path.exists(config_file) and os.path.exists(preprocessor_file):
            print(f"âœ“ Using local CLIP model: {local_path}")
            return local_path
        else:
            print(f"âš ï¸ Local path exists but missing config files")
    
    print(f"âœ“ Using online CLIP model: {online_path}")
    print("Note: First run will download and cache the model")
    return online_path

# --- æ¨¡åž‹åŠ è½½å‡½æ•° ---
def load_trained_clip_lora_model(model_path, lora_path=None):
    """
    åŠ è½½è®­ç»ƒå¥½çš„å¢žå¼ºCLIP + LoRAæ¨¡åž‹
    """
    if not os.path.exists(model_path):
        raise FileNotFoundError(f"Model file not found: {model_path}")
    
    print(f"Loading Enhanced CLIP + LoRA model from: {model_path}")
    
    # èŽ·å–CLIPæ¨¡åž‹è·¯å¾„
    clip_model_name = get_clip_model_path()
    
    # åˆ›å»ºæ¨¡åž‹ç»“æž„ï¼ˆä¸Žè®­ç»ƒæ—¶ä¿æŒä¸€è‡´ï¼‰
    model = CLIPLoRAClassifier(
        clip_model_name=clip_model_name,
        num_classes=NUM_CLASSES,
        lora_config=LORA_CONFIG,
        load_lora_from_path=lora_path  # å¦‚æžœæä¾›LoRAè·¯å¾„ï¼Œç›´æŽ¥åŠ è½½
    )
    
    # åŠ è½½è®­ç»ƒå¥½çš„æƒé‡
    checkpoint = torch.load(model_path, map_location=DEVICE)
    model.load_state_dict(checkpoint)
    
    # ç§»åŠ¨åˆ°è®¾å¤‡å¹¶è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
    model = model.to(DEVICE)
    model.eval()
    
    print("Enhanced CLIP + LoRA model loaded successfully!")
    return model

# --- å›¾åƒé¢„å¤„ç†å‡½æ•° ---
def get_clip_processor():
    """
    èŽ·å–CLIPå¤„ç†å™¨ï¼ˆä¸Žè®­ç»ƒæ—¶ä¿æŒä¸€è‡´ï¼‰
    """
    clip_model_name = get_clip_model_path()
    return CLIPProcessor.from_pretrained(clip_model_name)

def get_clip_transforms():
    """
    èŽ·å–CLIPçš„æ•°æ®å¢žå¼ºå˜æ¢ï¼ˆç”¨äºŽæŽ¨ç†æ—¶çš„é¢å¤–é¢„å¤„ç†ï¼‰
    """
    return transforms.Compose([
        transforms.Resize(256),
        transforms.CenterCrop(224),  # CLIPä½¿ç”¨224x224
        # æ³¨æ„ï¼šä¸åœ¨è¿™é‡Œè½¬æ¢ä¸ºtensorï¼Œå› ä¸ºCLIPProcessorä¼šå¤„ç†
    ])

def preprocess_image(image_path):
    """
    é¢„å¤„ç†è¾“å…¥å›¾åƒï¼ˆä½¿ç”¨CLIPå¤„ç†å™¨ï¼‰
    """
    if not os.path.exists(image_path):
        raise FileNotFoundError(f"Image file not found: {image_path}")
    
    # åŠ è½½å›¾åƒ
    try:
        image = Image.open(image_path).convert('RGB')
        print(f"Loaded image: {image_path}")
        print(f"Original image size: {image.size}")
    except Exception as e:
        raise ValueError(f"Cannot load image {image_path}: {e}")
    
    # åº”ç”¨åŸºæœ¬å˜æ¢ï¼ˆå¯é€‰çš„æ•°æ®å¢žå¼ºï¼‰
    transform = get_clip_transforms()
    image = transform(image)
    
    # ä½¿ç”¨CLIPå¤„ç†å™¨è¿›è¡Œæœ€ç»ˆé¢„å¤„ç†
    processor = get_clip_processor()
    processed = processor(images=image, return_tensors="pt")
    image_tensor = processed['pixel_values']  # [1, 3, 224, 224]
    
    return image_tensor

# --- æµ‹è¯•æ—¶å¢žå¼º (TTA) é¢„æµ‹å‡½æ•° ---
def predict_with_tta(model, image_tensor, num_tta=8):
    """
    ä½¿ç”¨æµ‹è¯•æ—¶å¢žå¼ºæé«˜é¢„æµ‹ç¨³å®šæ€§å’Œå‡†ç¡®æ€§
    """
    model.eval()
    all_outputs = []
    
    with torch.no_grad():
        # åŽŸå§‹é¢„æµ‹
        outputs = model(image_tensor.to(DEVICE))
        all_outputs.append(outputs)
        
        # TTAé¢„æµ‹
        for _ in range(num_tta):
            # åˆ›å»ºå˜æ¢åŽçš„å›¾åƒå‰¯æœ¬
            tta_image = image_tensor.clone()
            
            # éšæœºæ°´å¹³ç¿»è½¬
            if torch.rand(1) > 0.5:
                tta_image = torch.flip(tta_image, dims=[3])
            
            # éšæœºåž‚ç›´ç¿»è½¬ï¼ˆåŒ»å­¦å›¾åƒé€‚ç”¨ï¼‰
            if torch.rand(1) > 0.7:
                tta_image = torch.flip(tta_image, dims=[2])
            
            # è½»å¾®çš„äº®åº¦è°ƒæ•´
            if torch.rand(1) > 0.6:
                brightness_factor = 0.9 + torch.rand(1) * 0.2  # 0.9-1.1
                tta_image = tta_image * brightness_factor
                tta_image = torch.clamp(tta_image, 0, 1)
            
            outputs = model(tta_image.to(DEVICE))
            all_outputs.append(outputs)
    
    # å¹³å‡æ‰€æœ‰é¢„æµ‹
    avg_outputs = torch.stack(all_outputs).mean(dim=0)
    probabilities = torch.nn.functional.softmax(avg_outputs, dim=1)
    
    _, predicted = torch.max(avg_outputs, 1)
    predicted_class_idx = predicted.item()
    predicted_class = CLASSES[predicted_class_idx]
    confidence = probabilities[0][predicted_class_idx].item()
    
    return predicted_class, confidence, probabilities[0].cpu().numpy()

# --- æ ‡å‡†é¢„æµ‹å‡½æ•° ---
def predict_image(model, image_tensor):
    """
    ä½¿ç”¨å¢žå¼ºCLIP + LoRAæ¨¡åž‹å¯¹å›¾åƒè¿›è¡Œé¢„æµ‹
    """
    image_tensor = image_tensor.to(DEVICE)
    
    with torch.no_grad():
        # å‰å‘ä¼ æ’­
        outputs = model(image_tensor)
        
        # èŽ·å–é¢„æµ‹æ¦‚çŽ‡
        probabilities = torch.nn.functional.softmax(outputs, dim=1)
        
        # èŽ·å–é¢„æµ‹ç±»åˆ«
        _, predicted = torch.max(outputs, 1)
        predicted_class_idx = predicted.item()
        predicted_class = CLASSES[predicted_class_idx]
        confidence = probabilities[0][predicted_class_idx].item()
        
        return predicted_class, confidence, probabilities[0].cpu().numpy()

def print_prediction_results(predicted_class, confidence, all_probabilities, use_tta=False):
    """
    æ‰“å°é¢„æµ‹ç»“æžœ
    """
    tta_text = " (with TTA)" if use_tta else ""
    print("\n" + "="*60)
    print(f"Enhanced CLIP + LoRA æ¨¡åž‹é¢„æµ‹ç»“æžœ{tta_text}")
    print(f"Enhanced CLIP + LoRA Model Prediction Results{tta_text}")
    print("="*60)
    
    # ä¸»è¦é¢„æµ‹ç»“æžœ
    class_name_cn = CLASS_NAMES_CN.get(predicted_class, predicted_class)
    print(f"é¢„æµ‹ç±»åˆ«: {class_name_cn} ({predicted_class})")
    print(f"ç½®ä¿¡åº¦: {confidence:.4f} ({confidence*100:.2f}%)")
    
    print("\næ‰€æœ‰ç±»åˆ«çš„æ¦‚çŽ‡åˆ†å¸ƒ:")
    print("-" * 30)
    for i, (class_name, prob) in enumerate(zip(CLASSES, all_probabilities)):
        class_name_cn = CLASS_NAMES_CN.get(class_name, class_name)
        print(f"{class_name_cn:8s} ({class_name:8s}): {prob:.4f} ({prob*100:.2f}%)")
    
    print("="*60)

# --- ä¸»å‡½æ•° ---
def main(image_path, use_tta=True):
    """
    ä¸»é¢„æµ‹å‡½æ•°
    """
    try:
        # 1. æ£€æŸ¥LoRAæƒé‡æ˜¯å¦å­˜åœ¨
        lora_path = None
        if os.path.exists(LORA_WEIGHTS_PATH):
            lora_path = LORA_WEIGHTS_PATH
            print(f"âœ“ Found LoRA weights at: {lora_path}")
        else:
            print(f"âš ï¸ LoRA weights not found at: {LORA_WEIGHTS_PATH}")
            print("Will load LoRA weights from the main model file")
        
        # 2. åŠ è½½è®­ç»ƒå¥½çš„å¢žå¼ºCLIP + LoRAæ¨¡åž‹
        print("Step 1: Loading trained Enhanced CLIP + LoRA model...")
        model = load_trained_clip_lora_model(MODEL_PATH, lora_path)
        
        # 3. é¢„å¤„ç†å›¾åƒ
        print("\nStep 2: Preprocessing image with CLIP processor...")
        image_tensor = preprocess_image(image_path)
        
        # 4. è¿›è¡Œé¢„æµ‹
        print(f"\nStep 3: Making prediction with Enhanced CLIP + LoRA model...")
        if use_tta:
            print("Using Test Time Augmentation (TTA) for improved accuracy...")
            predicted_class, confidence, all_probabilities = predict_with_tta(model, image_tensor)
        else:
            print("Using standard prediction...")
            predicted_class, confidence, all_probabilities = predict_image(model, image_tensor)
        
        # 5. æ˜¾ç¤ºç»“æžœ
        print_prediction_results(predicted_class, confidence, all_probabilities, use_tta)
        
        return predicted_class, confidence
        
    except Exception as e:
        print(f"Error during Enhanced CLIP + LoRA prediction: {e}")
        import traceback
        traceback.print_exc()
        return None, None

# --- æ‰¹é‡é¢„æµ‹å‡½æ•° ---
def predict_multiple_images(image_paths, use_tta=True):
    """
    å¯¹å¤šå¼ å›¾ç‰‡è¿›è¡Œæ‰¹é‡é¢„æµ‹
    """
    print("Loading Enhanced CLIP + LoRA model for batch prediction...")
    
    # æ£€æŸ¥LoRAæƒé‡
    lora_path = LORA_WEIGHTS_PATH if os.path.exists(LORA_WEIGHTS_PATH) else None
    model = load_trained_clip_lora_model(MODEL_PATH, lora_path)
    
    results = []
    for i, image_path in enumerate(image_paths):
        print(f"\n--- Processing image {i+1}/{len(image_paths)}: {os.path.basename(image_path)} ---")
        try:
            image_tensor = preprocess_image(image_path)
            
            if use_tta:
                predicted_class, confidence, all_probabilities = predict_with_tta(model, image_tensor)
            else:
                predicted_class, confidence, all_probabilities = predict_image(model, image_tensor)
            
            print_prediction_results(predicted_class, confidence, all_probabilities, use_tta)
            results.append({
                'image_path': image_path,
                'predicted_class': predicted_class,
                'confidence': confidence
            })
        except Exception as e:
            print(f"Error processing {image_path}: {e}")
            results.append({
                'image_path': image_path,
                'predicted_class': None,
                'confidence': None,
                'error': str(e)
            })
    
    return results

# --- æ¨¡åž‹æ¯”è¾ƒå‡½æ•° ---
def compare_predictions(image_path):
    """
    æ¯”è¾ƒæ ‡å‡†é¢„æµ‹å’ŒTTAé¢„æµ‹çš„ç»“æžœ
    """
    print("=== Enhanced Model Prediction Comparison ===")
    
    # æ ‡å‡†é¢„æµ‹
    print("\n--- Standard Prediction ---")
    standard_class, standard_conf = main(image_path, use_tta=False)
    
    # TTAé¢„æµ‹
    print("\n--- TTA Prediction ---")
    tta_class, tta_conf = main(image_path, use_tta=True)
    
    # æ¯”è¾ƒç»“æžœ
    print("\n" + "="*50)
    print("é¢„æµ‹ç»“æžœæ¯”è¾ƒ / Prediction Comparison")
    print("="*50)
    if standard_class and tta_class:
        print(f"æ ‡å‡†é¢„æµ‹: {CLASS_NAMES_CN.get(standard_class, standard_class)} (ç½®ä¿¡åº¦: {standard_conf:.4f})")
        print(f"TTAé¢„æµ‹:  {CLASS_NAMES_CN.get(tta_class, tta_class)} (ç½®ä¿¡åº¦: {tta_conf:.4f})")
        
        if standard_class == tta_class:
            print("âœ“ ä¸¤ç§æ–¹æ³•é¢„æµ‹ç»“æžœä¸€è‡´")
            if tta_conf > standard_conf:
                print(f"âœ“ TTAæé«˜äº†ç½®ä¿¡åº¦ (+{tta_conf - standard_conf:.4f})")
        else:
            print("âš ï¸ ä¸¤ç§æ–¹æ³•é¢„æµ‹ç»“æžœä¸ä¸€è‡´ï¼Œå»ºè®®ä½¿ç”¨TTAç»“æžœ")
    
    return standard_class, standard_conf, tta_class, tta_conf

# --- æ¨¡åž‹ä¿¡æ¯å‡½æ•° ---
def print_model_info():
    """
    æ‰“å°å¢žå¼ºæ¨¡åž‹ä¿¡æ¯
    """
    print("=== Enhanced CLIP + LoRA Model Information ===")
    print(f"Base CLIP model: {get_clip_model_path()}")
    print(f"Enhanced LoRA configuration:")
    print(f"  - Rank (r): {LORA_CONFIG['r']}")
    print(f"  - Alpha: {LORA_CONFIG['lora_alpha']}")
    print(f"  - Dropout: {LORA_CONFIG['lora_dropout']}")
    print(f"  - Target modules: {LORA_CONFIG['target_modules']}")
    print(f"Model weights: {MODEL_PATH}")
    print(f"LoRA weights: {LORA_WEIGHTS_PATH}")
    print(f"Classes: {CLASSES}")
    print(f"Enhanced Classification Head: 768 -> 1024 -> 512 -> 256 -> {NUM_CLASSES}")

# --- ä½¿ç”¨ç¤ºä¾‹ ---
if __name__ == "__main__":
    # æ‰“å°æ¨¡åž‹ä¿¡æ¯
    print_model_info()
    
    # å•å¼ å›¾ç‰‡é¢„æµ‹ç¤ºä¾‹
    image_path = "/Users/huangxh/Documents/DMECL/LC25000/lung_aca/lungaca2.jpeg"  # ä¿®æ”¹ä¸ºå®žé™…å›¾ç‰‡è·¯å¾„
    
    # æ£€æŸ¥å¿…è¦çš„åº“
    try:
        import transformers
        import peft
        print(f"\nâœ“ Transformers version: {transformers.__version__}")
        print(f"âœ“ PEFT version: {peft.__version__}")
    except ImportError as e:
        print(f"âŒ Missing library: {e}")
        print("Please install: pip install transformers peft")
        exit(1)
    
    # æ£€æŸ¥æ¨¡åž‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(MODEL_PATH):
        print(f"Error: Enhanced CLIP + LoRA model file not found at {MODEL_PATH}")
        print("Please make sure you have trained the Enhanced CLIP + LoRA model and saved it to the correct location.")
    else:
        # è¿›è¡Œé¢„æµ‹æ¯”è¾ƒ
        print("\n" + "="*70)
        print("Enhanced CLIP + LoRA é¢„æµ‹æ¼”ç¤º")
        print("="*70)
        
        # æ¯”è¾ƒæ ‡å‡†é¢„æµ‹å’ŒTTAé¢„æµ‹
        standard_class, standard_conf, tta_class, tta_conf = compare_predictions(image_path)
        
        if tta_class:
            print(f"\nðŸŽ‰ æŽ¨èç»“æžœ (TTA): {CLASS_NAMES_CN.get(tta_class, tta_class)} (ç½®ä¿¡åº¦: {tta_conf:.4f})")
    
    # æ‰¹é‡é¢„æµ‹ç¤ºä¾‹ï¼ˆå¯é€‰ï¼‰
    print("\n" + "="*70)
    print("æ‰¹é‡é¢„æµ‹ç¤ºä¾‹ / Batch Prediction Example")
    print("="*70)
    image_list = [
        "/Users/huangxh/Documents/DMECL/LC25000/lung_aca/lungaca8.jpeg", 
        "/Users/huangxh/Documents/DMECL/LC25000/lung_n/lungn10.jpeg", 
        "/Users/huangxh/Documents/DMECL/LC25000/lung_scc/lungscc14.jpeg"
    ]
    
    if os.path.exists(MODEL_PATH):
        print("ä½¿ç”¨TTAè¿›è¡Œæ‰¹é‡é¢„æµ‹...")
        results = predict_multiple_images(image_list, use_tta=True)
        
        # æ‰“å°æ‰¹é‡é¢„æµ‹æ±‡æ€»
        print("\n" + "="*60)
        print("æ‰¹é‡é¢„æµ‹æ±‡æ€» / Batch Prediction Summary (with TTA)")
        print("="*60)
        for result in results:
            if result.get('predicted_class'):
                class_cn = CLASS_NAMES_CN.get(result['predicted_class'], result['predicted_class'])
                print(f"{os.path.basename(result['image_path']):20s} -> {class_cn} ({result['confidence']:.4f})")
            else:
                print(f"{os.path.basename(result['image_path']):20s} -> Error: {result.get('error', 'Unknown')}")