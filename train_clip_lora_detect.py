import os
import sys
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
from torchvision import transforms
import numpy as np
from PIL import Image
from sklearn.model_selection import train_test_split
from tqdm import tqdm
import time
import copy
import multiprocessing
import warnings
import glob
import torch.nn.functional as F

# æ–°å¢ï¼šå¯¼å…¥transformersåº“å’ŒPEFTåº“ç”¨äºCLIPæ¨¡å‹å’ŒLoRA
from transformers import CLIPModel, CLIPProcessor
from peft import LoraConfig, get_peft_model, TaskType

warnings.filterwarnings('ignore')

# --- 1. ä¼˜åŒ–åçš„é…ç½®å‚æ•° ---
# è·¯å¾„è®¾ç½®
BASE_DIR = "./lung_cancer_data"
MODEL_SAVE_PATH = os.path.join(BASE_DIR, "clip_lora_best_model.pth")

# æœ¬åœ°æ•°æ®é›†è·¯å¾„
LOCAL_DATASET_PATH = "/Users/huangxh/Documents/DMECL/LC25000"

# ä¼˜åŒ–åçš„è®­ç»ƒå‚æ•°
BATCH_SIZE = 16  # ä¿æŒä¸å˜
NUM_EPOCHS = 15  # å¢åŠ è®­ç»ƒè½®æ•°
LEARNING_RATE = 0.0005  # é™ä½å­¦ä¹ ç‡
PATIENCE = 8  # å¢åŠ è€å¿ƒå€¼

# CLIPæ¨¡å‹é…ç½®
LOCAL_CLIP_MODEL_PATH = "/Users/huangxh/Documents/DMECL/clip-vit-base-patch32-local"

# æ–¹æ¡ˆ3ï¼šä¼˜åŒ–åçš„LoRAé…ç½®å‚æ•°
LORA_CONFIG = {
    "r": 32,  # å¢åŠ LoRA rank
    "lora_alpha": 64,  # å¢åŠ alphaå€¼
    "lora_dropout": 0.05,  # é™ä½dropout
    "target_modules": [
        "q_proj", "k_proj", "v_proj", "out_proj",  # attention layers
        "fc1", "fc2"  # feed forward layers
    ]
}

# è®¾å¤‡æ£€æµ‹
def get_device():
    """
    è·å–æœ€ä½³å¯ç”¨è®¾å¤‡ï¼Œä¼˜å…ˆçº§ï¼šMPS > CUDA > CPU
    """
    if torch.backends.mps.is_available():
        return torch.device("mps")
    elif torch.cuda.is_available():
        return torch.device("cuda")
    else:
        return torch.device("cpu")

DEVICE = get_device()

# LC25000æ•°æ®é›†çš„ç±»åˆ«
CLASSES = ['lung_aca', 'lung_n', 'lung_scc']  # è‚ºè…ºç™Œã€æ­£å¸¸ã€è‚ºé³ç™Œ
NUM_CLASSES = len(CLASSES)

print(f"Using device: {DEVICE}")

# --- æ–¹æ¡ˆ2ï¼šæŸå¤±å‡½æ•°ä¼˜åŒ– ---
class LabelSmoothingCrossEntropy(nn.Module):
    """
    Label Smoothingäº¤å‰ç†µæŸå¤±ï¼Œæœ‰åŠ©äºæé«˜æ³›åŒ–èƒ½åŠ›
    """
    def __init__(self, smoothing=0.1):
        super(LabelSmoothingCrossEntropy, self).__init__()
        self.smoothing = smoothing
        
    def forward(self, pred, target):
        confidence = 1. - self.smoothing
        logprobs = F.log_softmax(pred, dim=-1)
        nll_loss = -logprobs.gather(dim=-1, index=target.unsqueeze(1))
        nll_loss = nll_loss.squeeze(1)
        smooth_loss = -logprobs.mean(dim=-1)
        loss = confidence * nll_loss + self.smoothing * smooth_loss
        return loss.mean()

class FocalLoss(nn.Module):
    """
    Focal Lossï¼Œå¸®åŠ©æ¨¡å‹å…³æ³¨éš¾åˆ†ç±»æ ·æœ¬
    """
    def __init__(self, alpha=1, gamma=2, reduction='mean'):
        super(FocalLoss, self).__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.reduction = reduction

    def forward(self, inputs, targets):
        ce_loss = F.cross_entropy(inputs, targets, reduction='none')
        pt = torch.exp(-ce_loss)
        focal_loss = self.alpha * (1-pt)**self.gamma * ce_loss
        
        if self.reduction == 'mean':
            return focal_loss.mean()
        return focal_loss

# --- æ–¹æ¡ˆ4ï¼šå¢å¼ºçš„CLIP + LoRA + åˆ†ç±»å¤´æ¨¡å‹å®šä¹‰ ---
class CLIPLoRAClassifier(nn.Module):
    """
    CLIP Vision Encoder + LoRA + å¢å¼ºåˆ†ç±»å¤´æ¨¡å‹
    """
    def __init__(self, clip_model_name=LOCAL_CLIP_MODEL_PATH, num_classes=NUM_CLASSES, 
                 lora_config=LORA_CONFIG):
        super(CLIPLoRAClassifier, self).__init__()
        
        # åŠ è½½åŸºç¡€CLIPæ¨¡å‹
        print("Loading base CLIP model...")
        self.base_clip_model = CLIPModel.from_pretrained(clip_model_name)
        self.processor = CLIPProcessor.from_pretrained(clip_model_name)
        
        # å†»ç»“æ‰€æœ‰CLIPå‚æ•°
        for param in self.base_clip_model.parameters():
            param.requires_grad = False
        print("âœ… All CLIP parameters frozen")
        
        # é…ç½®LoRA
        self.lora_config = LoraConfig(
            r=lora_config["r"],
            lora_alpha=lora_config["lora_alpha"],
            target_modules=lora_config["target_modules"],
            lora_dropout=lora_config["lora_dropout"],
            bias="none",
            task_type=TaskType.FEATURE_EXTRACTION,  # ç”¨äºç‰¹å¾æå–ä»»åŠ¡
        )
        
        # åº”ç”¨LoRAåˆ°CLIP vision model
        print("Applying LoRA to CLIP vision model...")
        self.clip_model = get_peft_model(self.base_clip_model, self.lora_config)
        
        # è·å–CLIP vision encoderçš„è¾“å‡ºç»´åº¦
        self.clip_hidden_size = self.base_clip_model.config.vision_config.hidden_size
        
        # æ–¹æ¡ˆ4ï¼šå¢å¼ºçš„åˆ†ç±»å¤´ - æ›´æ·±å±‚çš„ç½‘ç»œ
        self.classifier = nn.Sequential(
            nn.Dropout(0.1),
            nn.Linear(self.clip_hidden_size, 1024),
            nn.ReLU(),
            nn.BatchNorm1d(1024),
            nn.Dropout(0.2),
            nn.Linear(1024, 512),
            nn.ReLU(),
            nn.BatchNorm1d(512),
            nn.Dropout(0.1),
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(256, num_classes)
        )
        
        # æ‰“å°æ¨¡å‹ä¿¡æ¯
        self._print_model_info()
        
    def _print_model_info(self):
        """æ‰“å°æ¨¡å‹å‚æ•°ä¿¡æ¯"""
        total_params = sum(p.numel() for p in self.parameters())
        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)
        
        print(f"\n=== Enhanced Model Information ===")
        print(f"CLIP hidden size: {self.clip_hidden_size}")
        print(f"Enhanced Classification head: {self.clip_hidden_size} -> 1024 -> 512 -> 256 -> {NUM_CLASSES}")
        print(f"Total parameters: {total_params:,}")
        print(f"Trainable parameters: {trainable_params:,}")
        print(f"Trainable ratio: {trainable_params/total_params*100:.2f}%")
        
        # æ‰“å°LoRAé…ç½®
        print(f"\n=== Enhanced LoRA Configuration ===")
        print(f"LoRA rank (r): {self.lora_config.r}")
        print(f"LoRA alpha: {self.lora_config.lora_alpha}")
        print(f"LoRA dropout: {self.lora_config.lora_dropout}")
        print(f"Target modules: {self.lora_config.target_modules}")
        
        # æ‰“å°å¯è®­ç»ƒå‚æ•°è¯¦æƒ…
        print(f"\n=== Trainable Parameters ===")
        for name, param in self.named_parameters():
            if param.requires_grad:
                print(f"  {name}: {param.numel():,} parameters")
    
    def forward(self, pixel_values):
        # é€šè¿‡CLIP vision encoder (with LoRA) æå–ç‰¹å¾
        vision_outputs = self.clip_model.vision_model(pixel_values=pixel_values)
        
        # è·å–pooledè¾“å‡º (CLS tokençš„è¡¨ç¤º)
        pooled_output = vision_outputs.pooler_output  # [batch_size, hidden_size]
        
        # é€šè¿‡å¢å¼ºçš„åˆ†ç±»å¤´
        logits = self.classifier(pooled_output)
        
        return logits
    
    def save_lora_weights(self, save_path):
        """ä¿å­˜LoRAæƒé‡"""
        self.clip_model.save_pretrained(save_path)
        print(f"LoRA weights saved to: {save_path}")
    
    def load_lora_weights(self, load_path):
        """åŠ è½½LoRAæƒé‡"""
        from peft import PeftModel
        self.clip_model = PeftModel.from_pretrained(self.base_clip_model, load_path)
        print(f"LoRA weights loaded from: {load_path}")

# --- 3. å¢å¼ºçš„å›¾åƒé¢„å¤„ç†å‡½æ•° ---
def get_clip_transforms():
    """
    é’ˆå¯¹åŒ»å­¦å›¾åƒçš„å¢å¼ºæ•°æ®å¢å¼ºç­–ç•¥
    """
    data_transforms = {
        'train': transforms.Compose([
            transforms.Resize((256, 256)),
            transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),
            transforms.RandomHorizontalFlip(p=0.5),
            transforms.RandomVerticalFlip(p=0.3),  # åŒ»å­¦å›¾åƒå¯ä»¥å‚ç›´ç¿»è½¬
            transforms.RandomRotation(degrees=20),  # å¢åŠ æ—‹è½¬è§’åº¦
            transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.3, hue=0.1),
            transforms.RandomAffine(degrees=15, translate=(0.1, 0.1), scale=(0.9, 1.1)),
            # åŒ»å­¦å›¾åƒç‰¹å®šå¢å¼º
            transforms.RandomApply([
                transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 2.0))
            ], p=0.3),
        ]),
        'validation': transforms.Compose([
            transforms.Resize(256),
            transforms.CenterCrop(224)
        ]),
        'test': transforms.Compose([
            transforms.Resize(256),
            transforms.CenterCrop(224)
        ])
    }
    
    return data_transforms

# --- 4. ä¿®æ”¹åçš„æœ¬åœ°æ•°æ®é›†ç±» ---
class LocalImageDataset(Dataset):
    """
    æœ¬åœ°å›¾åƒæ•°æ®é›†ç±»ï¼Œé€‚é…CLIPè¾“å…¥æ ¼å¼
    """
    def __init__(self, image_paths, labels, transform=None, class_to_idx=None, processor=None):
        self.original_image_paths = image_paths.copy()
        self.original_labels = labels.copy()
        self.transform = transform
        self.class_to_idx = class_to_idx or {}
        self.processor = processor
        
        # éªŒè¯å¹¶è¿‡æ»¤æœ‰æ•ˆçš„å›¾åƒ
        self._validate_images()
        
    def _validate_images(self):
        """éªŒè¯å›¾åƒæ–‡ä»¶å¹¶ç§»é™¤æŸåçš„æ–‡ä»¶"""
        print("Validating image files...")
        valid_indices = []
        invalid_count = 0
        
        for idx, image_path in enumerate(tqdm(self.original_image_paths, desc="Validating images")):
            try:
                if not os.path.exists(image_path):
                    invalid_count += 1
                    continue
                    
                file_size = os.path.getsize(image_path)
                if file_size <= 200:
                    invalid_count += 1
                    continue
                
                try:
                    with Image.open(image_path) as img:
                        _ = img.size
                        _ = img.mode
                    valid_indices.append(idx)
                except Exception:
                    invalid_count += 1
                        
            except Exception as e:
                invalid_count += 1
        
        # æ›´æ–°æœ‰æ•ˆçš„å›¾åƒè·¯å¾„å’Œæ ‡ç­¾
        self.image_paths = [self.original_image_paths[i] for i in valid_indices]
        self.labels = [self.original_labels[i] for i in valid_indices]
        
        print(f"Validation complete: {len(self.image_paths)} valid images, {invalid_count} invalid images removed")
        
    def __len__(self):
        return len(self.image_paths)
    
    def __getitem__(self, idx):
        # åŠ è½½å›¾åƒ
        image_path = self.image_paths[idx]
        
        try:
            image = Image.open(image_path).convert('RGB')
        except Exception as e:
            print(f"Warning: Cannot load image {image_path}: {e}")
            image = Image.new('RGB', (224, 224), color=(128, 128, 128))
        
        # è·å–æ ‡ç­¾
        label = self.labels[idx]
        
        # åº”ç”¨å˜æ¢
        if self.transform:
            try:
                image = self.transform(image)
            except Exception as e:
                print(f"Transform failed for {image_path}: {e}")
                image = torch.zeros(3, 224, 224)
        
        if self.processor:
            try:
                # CLIPProcessorä¼šå°†PILå›¾åƒè½¬æ¢ä¸ºtensorå¹¶è¿›è¡Œæ ‡å‡†åŒ–
                processed = self.processor(images=image, return_tensors="pt")
                image = processed['pixel_values'].squeeze(0)  # ç§»é™¤batchç»´åº¦
            except Exception as e:
                print(f"CLIPProcessor failed for {image_path}: {e}")
                image = torch.zeros(3, 224, 224)
        else:
            # å¦‚æœæ²¡æœ‰processorï¼Œä½¿ç”¨é»˜è®¤çš„tensorè½¬æ¢
            if not isinstance(image, torch.Tensor):
                image = transforms.ToTensor()(image)

        # å¤„ç†æ ‡ç­¾
        if isinstance(label, str) and self.class_to_idx:
            label = self.class_to_idx.get(label, 0)
        
        return image, label

# --- 5. æ•°æ®åŠ è½½å‡½æ•°ï¼ˆä¿æŒåŸºæœ¬ä¸å˜ï¼‰ ---
def load_local_dataset():
    """ä»æœ¬åœ°ç›®å½•åŠ è½½LC25000æ•°æ®é›†"""
    print("--- Loading local LC25000 dataset ---")
    
    if not os.path.exists(LOCAL_DATASET_PATH):
        raise FileNotFoundError(f"Dataset directory not found: {LOCAL_DATASET_PATH}")
    
    image_paths = []
    labels = []
    class_to_idx = {}
    
    image_extensions = ['*.jpg', '*.jpeg', '*.png', '*.bmp', '*.tiff']
    
    class_dirs = [d for d in os.listdir(LOCAL_DATASET_PATH) 
                  if os.path.isdir(os.path.join(LOCAL_DATASET_PATH, d)) and not d.startswith('.')]
    
    if class_dirs:
        print("Found class directories:", class_dirs)
        
        class_to_idx = {cls: idx for idx, cls in enumerate(sorted(class_dirs))}
        idx_to_class = {idx: cls for cls, idx in class_to_idx.items()}
        
        for class_name in class_dirs:
            class_dir = os.path.join(LOCAL_DATASET_PATH, class_name)
            
            class_images = []
            for ext in image_extensions:
                class_images.extend(glob.glob(os.path.join(class_dir, ext)))
            
            print(f"Found {len(class_images)} potential images in class '{class_name}'")
            
            valid_images = []
            for img_path in class_images:
                if os.path.getsize(img_path) > 200:
                    valid_images.append(img_path)
            
            print(f"Valid images in class '{class_name}': {len(valid_images)}")
            
            image_paths.extend(valid_images)
            labels.extend([class_name] * len(valid_images))
    
    print(f"Total images loaded: {len(image_paths)}")
    print(f"Classes: {list(class_to_idx.keys())}")
    
    return image_paths, labels, class_to_idx, idx_to_class

def create_data_splits_local(image_paths, labels, class_to_idx):
    """å°†æœ¬åœ°æ•°æ®é›†åˆ†å‰²ä¸ºè®­ç»ƒã€éªŒè¯å’Œæµ‹è¯•é›†"""
    print("--- Creating data splits ---")
    
    indices = list(range(len(image_paths)))
    
    try:
        numeric_labels = [class_to_idx[label] if isinstance(label, str) else label 
                         for label in labels]
        
        train_indices, temp_indices = train_test_split(
            indices, test_size=0.3, random_state=42, stratify=numeric_labels
        )
        
        temp_labels = [numeric_labels[i] for i in temp_indices]
        val_indices, test_indices = train_test_split(
            temp_indices, test_size=0.5, random_state=42, stratify=temp_labels
        )
        
    except Exception as e:
        print(f"Stratified split failed: {e}, using random split")
        train_indices, temp_indices = train_test_split(
            indices, test_size=0.3, random_state=42
        )
        val_indices, test_indices = train_test_split(
            temp_indices, test_size=0.5, random_state=42
        )
    
    print(f"Train samples: {len(train_indices)}")
    print(f"Validation samples: {len(val_indices)}")
    print(f"Test samples: {len(test_indices)}")
    
    splits = {}
    for split_name, indices in [('train', train_indices), 
                               ('validation', val_indices), 
                               ('test', test_indices)]:
        splits[split_name] = {
            'image_paths': [image_paths[i] for i in indices],
            'labels': [labels[i] for i in indices]
        }
    
    return splits

def create_data_loaders_local():
    """åˆ›å»ºæœ¬åœ°æ•°æ®é›†çš„æ•°æ®åŠ è½½å™¨ï¼Œä½¿ç”¨CLIPé¢„å¤„ç†"""
    print("--- Creating data loaders ---")
    
    image_paths, labels, class_to_idx, idx_to_class = load_local_dataset()
    
    if len(image_paths) == 0:
        print("No valid images found. Please download Git LFS files first.")
        return None, None, None, None, None, None
    
    data_splits = create_data_splits_local(image_paths, labels, class_to_idx)
    
    processor = CLIPProcessor.from_pretrained(LOCAL_CLIP_MODEL_PATH)
    data_transforms = get_clip_transforms()
    
    pytorch_datasets = {}
    for split in ['train', 'validation', 'test']:
        pytorch_datasets[split] = LocalImageDataset(
            data_splits[split]['image_paths'],
            data_splits[split]['labels'],
            data_transforms.get(split, data_transforms['validation']),
            class_to_idx,
            processor
        )
    
    dataloaders = {}
    for split in ['train', 'validation', 'test']:
        dataloaders[split] = DataLoader(
            pytorch_datasets[split],
            batch_size=BATCH_SIZE,
            shuffle=(split == 'train'),
            num_workers=0,
            pin_memory=torch.cuda.is_available(),
            drop_last=False
        )
    
    dataset_sizes = {split: len(pytorch_datasets[split]) for split in ['train', 'validation', 'test']}
    
    return pytorch_datasets, dataloaders, dataset_sizes, class_to_idx, idx_to_class, data_splits

# --- 6. æ¨¡å‹ç›¸å…³å‡½æ•° ---
def load_clip_lora_model():
    """åŠ è½½CLIP + LoRA + åˆ†ç±»å¤´æ¨¡å‹"""
    print(f"Loading enhanced CLIP + LoRA model: {LOCAL_CLIP_MODEL_PATH}")
    model = CLIPLoRAClassifier(
        clip_model_name=LOCAL_CLIP_MODEL_PATH,
        num_classes=NUM_CLASSES,
        lora_config=LORA_CONFIG
    )
    return model

# --- 7. æµ‹è¯•å‡½æ•° ---
def test_model(model, test_dataloader, dataset_size, class_to_idx, idx_to_class):
    """
    æµ‹è¯•è®­ç»ƒå¥½çš„CLIP + LoRAæ¨¡å‹
    """
    print("\n--- Testing Enhanced CLIP + LoRA model on test dataset ---")
    
    model.eval()
    
    running_corrects = 0
    all_preds = []
    all_labels = []
    
    # ç”¨äºè®¡ç®—æ¯ä¸ªç±»åˆ«çš„å‡†ç¡®ç‡
    class_correct = {i: 0 for i in range(len(class_to_idx))}
    class_total = {i: 0 for i in range(len(class_to_idx))}
    
    with torch.no_grad():
        pbar = tqdm(test_dataloader, desc="Testing Enhanced CLIP + LoRA")
        
        for inputs, labels in pbar:
            inputs = inputs.to(DEVICE)
            labels = labels.to(DEVICE)
            
            # å‰å‘ä¼ æ’­
            outputs = model(inputs)
            _, preds = torch.max(outputs, 1)
            
            # ç»Ÿè®¡æ­£ç¡®é¢„æµ‹
            running_corrects += torch.sum(preds == labels.data)
            
            # ä¿å­˜é¢„æµ‹ç»“æœç”¨äºè¯¦ç»†åˆ†æ
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
            
            # ç»Ÿè®¡æ¯ä¸ªç±»åˆ«çš„å‡†ç¡®ç‡
            for i in range(labels.size(0)):
                label = labels[i].item()
                pred = preds[i].item()
                class_total[label] += 1
                if label == pred:
                    class_correct[label] += 1
    
    # è®¡ç®—æ€»ä½“å‡†ç¡®ç‡
    test_acc = running_corrects.item() / dataset_size
    
    print(f"\n=== Enhanced CLIP + LoRA Test Results ===")
    print(f"Overall Test Accuracy: {test_acc:.4f} ({running_corrects}/{dataset_size})")
    
    # æ‰“å°æ¯ä¸ªç±»åˆ«çš„å‡†ç¡®ç‡
    print(f"\nPer-class Accuracy:")
    for class_idx, class_name in idx_to_class.items():
        if class_total[class_idx] > 0:
            class_acc = class_correct[class_idx] / class_total[class_idx]
            print(f"  {class_name}: {class_acc:.4f} ({class_correct[class_idx]}/{class_total[class_idx]})")
        else:
            print(f"  {class_name}: No samples in test set")
    
    return test_acc, all_preds, all_labels

# --- 8. ä¼˜åŒ–åçš„è®­ç»ƒå‡½æ•° ---
def train_model():
    """ä½¿ç”¨ä¼˜åŒ–åçš„CLIP + LoRAæ¨¡å‹è®­ç»ƒæœ¬åœ°LC25000æ•°æ®é›†"""
    print("\n--- Training with Enhanced CLIP + LoRA ---")
    print(f"Using device: {DEVICE}")
    print(f"CLIP model: {LOCAL_CLIP_MODEL_PATH}")

    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    result = create_data_loaders_local()
    if result[0] is None:
        print("Error: Failed to create data loaders")
        return None
    
    pytorch_datasets, dataloaders, dataset_sizes, class_to_idx, idx_to_class, data_splits = result
    
    if dataset_sizes['train'] == 0:
        print("Error: No training data found.")
        return None

    print(f"Dataset sizes: {dataset_sizes}")
    print(f"Classes: {list(class_to_idx.keys())}")

    # åŠ è½½å¢å¼ºçš„CLIP + LoRAæ¨¡å‹
    model = load_clip_lora_model()
    model = model.to(DEVICE)

    # æ–¹æ¡ˆ2ï¼šç»„åˆæŸå¤±å‡½æ•°
    criterion_ce = nn.CrossEntropyLoss()
    criterion_ls = LabelSmoothingCrossEntropy(smoothing=0.1)
    criterion_focal = FocalLoss(alpha=1, gamma=2)
    
    def combined_loss(outputs, labels):
        loss_ce = criterion_ce(outputs, labels)
        loss_ls = criterion_ls(outputs, labels)
        loss_focal = criterion_focal(outputs, labels)
        return 0.4 * loss_ce + 0.3 * loss_ls + 0.3 * loss_focal

    # æ–¹æ¡ˆ3ï¼šåˆ†ç»„å‚æ•°ï¼Œä½¿ç”¨ä¸åŒå­¦ä¹ ç‡
    lora_params = []
    classifier_params = []
    
    for name, param in model.named_parameters():
        if param.requires_grad:
            if 'lora' in name.lower():
                lora_params.append(param)
            else:
                classifier_params.append(param)
    
    # åˆ†ç±»å¤´ä½¿ç”¨æ›´é«˜å­¦ä¹ ç‡ï¼ŒLoRAä½¿ç”¨è¾ƒä½å­¦ä¹ ç‡
    optimizer = optim.AdamW([
        {'params': lora_params, 'lr': LEARNING_RATE * 0.5},  # LoRAè¾ƒä½å­¦ä¹ ç‡
        {'params': classifier_params, 'lr': LEARNING_RATE * 2}  # åˆ†ç±»å¤´æ›´é«˜å­¦ä¹ ç‡
    ], weight_decay=1e-4)
    
    print("âœ… Using differentiated learning rates:")
    print(f"  LoRA parameters: {LEARNING_RATE * 0.5}")
    print(f"  Classifier parameters: {LEARNING_RATE * 2}")
    
    # ä½¿ç”¨ä½™å¼¦é€€ç«è°ƒåº¦å™¨
    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(
        optimizer, T_0=5, T_mult=2, eta_min=1e-6
    )

    # è®­ç»ƒå¾ªç¯
    best_model_wts = copy.deepcopy(model.state_dict())
    best_acc = 0.0
    patience_counter = 0
    
    # æ·»åŠ æ¢¯åº¦è£å‰ª
    max_grad_norm = 1.0

    print("Starting enhanced training...")
    start_time = time.time()

    for epoch in range(NUM_EPOCHS):
        print(f'Epoch {epoch+1}/{NUM_EPOCHS}')
        print('-' * 10)

        for phase in ['train', 'validation']:
            if phase == 'train':
                model.train()
            else:
                model.eval()

            running_loss = 0.0
            running_corrects = 0
            
            # è¯¦ç»†çš„ç±»åˆ«ç»Ÿè®¡
            class_corrects = {i: 0 for i in range(NUM_CLASSES)}
            class_totals = {i: 0 for i in range(NUM_CLASSES)}

            pbar = tqdm(dataloaders[phase], desc=f"Epoch {epoch+1} - {phase}")
            
            for inputs, labels in pbar:
                inputs = inputs.to(DEVICE)
                labels = labels.to(DEVICE)

                optimizer.zero_grad()

                with torch.set_grad_enabled(phase == 'train'):
                    try:
                        outputs = model(inputs)
                        loss = combined_loss(outputs, labels)  # ä½¿ç”¨ç»„åˆæŸå¤±
                        _, preds = torch.max(outputs, 1)

                        if phase == 'train':
                            loss.backward()
                            # æ¢¯åº¦è£å‰ª
                            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)
                            optimizer.step()

                    except RuntimeError as e:
                        print(f"Runtime error during {phase}: {e}")
                        continue

                running_loss += loss.item() * inputs.size(0)
                running_corrects += torch.sum(preds == labels.data)
                
                # ç»Ÿè®¡æ¯ä¸ªç±»åˆ«çš„å‡†ç¡®ç‡
                for i in range(labels.size(0)):
                    label = labels[i].item()
                    pred = preds[i].item()
                    class_totals[label] += 1
                    if label == pred:
                        class_corrects[label] += 1

            epoch_loss = running_loss / dataset_sizes[phase]
            epoch_acc = running_corrects.item() / dataset_sizes[phase]

            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')
            
            # æ‰“å°æ¯ä¸ªç±»åˆ«çš„å‡†ç¡®ç‡
            print(f'{phase} Per-class Accuracy:')
            for class_idx, class_name in idx_to_class.items():
                if class_totals[class_idx] > 0:
                    class_acc = class_corrects[class_idx] / class_totals[class_idx]
                    print(f'  {class_name}: {class_acc:.4f} ({class_corrects[class_idx]}/{class_totals[class_idx]})')

            if phase == 'validation':
                # è®¡ç®—å¹³å‡ç±»åˆ«å‡†ç¡®ç‡ï¼ˆæ›´å…³æ³¨å°‘æ•°ç±»åˆ«ï¼‰
                valid_class_accs = []
                for class_idx in range(NUM_CLASSES):
                    if class_totals[class_idx] > 0:
                        class_acc = class_corrects[class_idx] / class_totals[class_idx]
                        valid_class_accs.append(class_acc)
                
                avg_class_acc = sum(valid_class_accs) / len(valid_class_accs) if valid_class_accs else 0
                
                # ä½¿ç”¨å¹³å‡ç±»åˆ«å‡†ç¡®ç‡ä½œä¸ºä¿å­˜æ ‡å‡†
                if avg_class_acc > best_acc:
                    best_acc = avg_class_acc
                    best_model_wts = copy.deepcopy(model.state_dict())
                    
                    # ä¿å­˜å®Œæ•´æ¨¡å‹
                    os.makedirs(os.path.dirname(MODEL_SAVE_PATH), exist_ok=True)
                    torch.save(model.state_dict(), MODEL_SAVE_PATH)
                    
                    # å•ç‹¬ä¿å­˜LoRAæƒé‡
                    lora_save_path = os.path.join(BASE_DIR, "lora_weights")
                    model.save_lora_weights(lora_save_path)
                    
                    print(f"New best model saved with avg class accuracy: {best_acc:.4f}")
                    patience_counter = 0
                else:
                    patience_counter += 1

        if phase == 'train':
            scheduler.step()  # æ›´æ–°å­¦ä¹ ç‡

        if patience_counter >= PATIENCE:
            print(f"Early stopping triggered after {epoch+1} epochs")
            break

    total_time = time.time() - start_time
    print(f'Enhanced training completed in {total_time:.0f}s')
    print(f'Best average class accuracy: {best_acc:.4f}')

    model.load_state_dict(best_model_wts)
    return model, class_to_idx, idx_to_class, dataloaders

# --- 9. ä¸»å‡½æ•° ---
def main():
    """ä¸»å‡½æ•°"""
    try:
        print("=== Enhanced CLIP + LoRA LC25000 Lung Cancer Classification ===")
        print(f"PyTorch version: {torch.__version__}")
        print(f"Device: {DEVICE}")
        print(f"CLIP model: {LOCAL_CLIP_MODEL_PATH}")
        print(f"Local dataset path: {LOCAL_DATASET_PATH}")
        print()

        # æ£€æŸ¥å¿…è¦çš„åº“
        try:
            import transformers
            import peft
            print(f"âœ“ Transformers version: {transformers.__version__}")
            print(f"âœ“ PEFT version: {peft.__version__}")
        except ImportError as e:
            print(f"âŒ Missing library: {e}")
            print("Please install: pip install transformers peft")
            return

        # æ£€æŸ¥æœ¬åœ°æ•°æ®é›†è·¯å¾„
        if not os.path.exists(LOCAL_DATASET_PATH):
            print(f"âœ— Dataset directory not found: {LOCAL_DATASET_PATH}")
            return
        else:
            print(f"âœ“ Dataset directory found: {LOCAL_DATASET_PATH}")

        # è®­ç»ƒæ¨¡å‹
        result = train_model()
        
        if result is not None:
            model, class_to_idx, idx_to_class, dataloaders = result
            print("Enhanced training completed successfully!")
            print(f"Model saved at: {MODEL_SAVE_PATH}")
            print(f"LoRA weights saved at: {os.path.join(BASE_DIR, 'lora_weights')}")
            print(f"Class mapping: {class_to_idx}")

            # åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹
            if 'test' in dataloaders:
                # é‡æ–°åˆ›å»ºæ•°æ®åŠ è½½å™¨ä»¥è·å–dataset_sizes
                _, _, dataset_sizes, _, _, _ = create_data_loaders_local()
                
                test_acc, test_preds, test_labels = test_model(
                    model, 
                    dataloaders['test'], 
                    dataset_sizes['test'], 
                    class_to_idx, 
                    idx_to_class
                )
                
                print(f"\nğŸ‰ Final Enhanced CLIP + LoRA Test Accuracy: {test_acc:.4f}")
            else:
                print("âŒ Test dataloader not available")
        else:
            print("Training failed!")
            return
        
        print("\n=== Enhanced Training completed! ===")
        
    except KeyboardInterrupt:
        print("\nTraining interrupted by user")
    except Exception as e:
        print(f"Error in main pipeline: {e}")
        import traceback
        traceback.print_exc()

if __name__ == '__main__':
    main()